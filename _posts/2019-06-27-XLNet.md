---
layout: post
title: XLNet
subtitle: Generalized Autoregressive Pretraining for Language Understanding
tags: [NLP, models, Word Embedding]
---

#XLNet outperforms #BERT on 20 tasks, often by a large margin, and achieves state-of-the-art results on 18 tasks including question answering, natural language inference, sentiment analysis, and document ranking. Code and comparisons here:

[github](https://github.com/zihangdai/xlnet)



[arxiv](https://arxiv.org/abs/1906.08237v1)
